% SIAM Article Template
\documentclass[final]{siamart1116}
% Other choices are [review]

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

% Packages and macros go here
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithmic}

% Weiqi added
\usepackage{amsmath}

\ifpdf
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi

%strongly recommended
\numberwithin{theorem}{section}

% Declare title and authors, without \thanks
\newcommand{\TheTitle}{Surrogate Subspace} 
\newcommand{\TheAuthors}{Weiqi Ji}

% Sets running headers as well as PDF title and authors
% \headers{\TheTitle}{\TheAuthors}

% Title. If the supplement option is on, then "Supplementary Material"
% is automatically inserted before the title.
% \title{{\TheTitle}\thanks{ 
% \funding{  }}}

\title{\TheTitle}

% Authors: full names plus addresses.
\author{
  Weiqi Ji\thanks{ 
    Email: \email{weiqiji@mit.edu}.}
}

\usepackage{amsopn}
\DeclareMathOperator{\diag}{diag}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={\TheTitle},
  pdfauthor={\TheAuthors}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

% \externaldocument{ex_supplement}

% FundRef data to be entered by SIAM
%<funding-group>
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  A surrogate subspace is developed to approximate the probability density function of multiple output with the same subspace. The report will present its definition and an approach to identify it by combining the active subspace.
\end{abstract}

% REQUIRED
\begin{keywords}
  Surrogate Subspace, Low-fidelity model
\end{keywords}

% REQUIRED
% \begin{AMS}
%   68Q25, 68R10, 68U05
% \end{AMS}

\section{Introduction}

There are mainly three challenges in propagating the chemical kinetic uncertainties into turbulent combustion simulations.
\begin{enumerate}
\item The input parameter space is high dimensional.
\item Evaluation of the turbulent combustion simulation is expensive.
\item The output parameter space is also high dimensional, which includes the temperature, species concentrations and these quantiles discretized in space and time. Although this is optional if only a few quantiles of interest (QOIs) in practical applications, such as the ignition delay, flame speed.
\end{enumerate}

A straightforward way is applying the exciting subspace method to reduce the dimension of the input parameter space, such as the active subspace which approximating the function with a low dimensional subspace and a low dimensional function. Ana active subspace can be identified for each output, and the combination of these active subspace will form a comprehensive subspace for all of the outputs, but with the cost of increasing the dimension of the subspace.

Recognizing that the goal is to achieve the probability density function (PDF) of output in the content of forward UQ. Instead of approximating the function itself, approximating the PDFs of outputs is specified as the goal of the subspace. This definition also enable us to approximating multiple outputs with the same subspace.

The remain challenge is that a certain number of samples (proportional to the dimension of the inputs) needed to evaluated to identify the active/surrogate subspace. 

Then  we borrow the idea from reaction mechanism reduction, in which a small size of mechanism is generated by remove unimportant reactions from the comprehensive detailed mechanism to make the turbulent combustion simulations tractable with complex reaction mechanism. The reduction is done with simple combustion problems, such as zero-D and one-D simulation. Here, we will train the surrogate subspace and provide the necessary conditions for the extrapolation of the surrogate subspace. This part is undergoing, I am trying to estimate the performance of the surrogate subspace for new output by estimating the covariance/correlation coefficient.

\section{Define the Problem} 

Let the random vector $\mathbf{x} \in \mathbb{R}^n$ represents the input parameters to the simulation, and follows the possibility density function (PDF) of $\rho_{\mathbf{x}}$.

A series of functions $ \mathbf{y} = \mathcal{F}(\mathbf{x}) = [ f_1,f_2,\dots,f_m](\mathbf{x})$ maps the simulation inputs to the outputs, in which $f_i:\mathbb{R}^n \to \mathbb{R}$ and $\mathbf{y} = [y_1, y_2, ...,  y_m] \in \mathbb{R}^m$. The task is to achieve the PDF of $\mathbf{y}$ through Monte Carlo integration, which is denoted as $\rho_{\mathbf{y}}$.

The baseline for achieving this goal would be identifying the active subspace for each function, and then combine them together. But this will make the dimension of the subspace larger. Next, we will first briefly introduce the active subspace, and then identify a linear combination of the active subspace for function series such that we can approximate $\mathbf{y}$ with the same subspace. 

\section{Active Subspace} 
Subspace method, e.g., active subspace method \cite{constantine2014active}, seeks a low-rank matrix S which projects $\mathbb{R}^n$ to a lower dimensional subspace $V$ with rank of $r \leq n$, such that the function $f$ can be approximated by a lower dimensional function $g: \mathbb{R}^r \to \mathbb{R}$
\begin{equation}
f(\mathbf{x}) \approx g( S^T \mathbf{x} )
\end{equation}
This work takes the active subspace method as an example, and one way to identify the active subspace is the eigenvalue decomposition of the following matrix $C$,

\begin{equation}
C = \int \nabla f \nabla f \rho_{\mathbf{x}} d\mathbf{x}
\end{equation}

The value of the eigenvalue $\lambda_i$ will indicate how much the function will change along the corresponding eigenvector $\mathbf{u_i}$. If there is a gap between $\lambda _r$ and $\lambda _{r+1}$, then the first $r$ eigenvectors can be selected as active directions, and the matrix $S = [\mathbf{u_1}, \dots,\mathbf{u_r}]$. The optimum $g$ for the approximation, subject to some measure, will be the expectation value of $f$ over all of $\mathbf{z} \in \mathbb{R}^n$ corresponding to the same projection as $\mathbf{x}$, 
i.e., $ \hat{g}(S^T \mathbf{x}) = \mathbf{E} [ f(\mathbf{z}); S^T \mathbf{z} = S^T \mathbf{x} ] $. 
Therefore, the Monte Carlo sampling has to sample over both the active and inactive directions in the content of Monte Carlo integration problem. However, if $\lambda_i >> \lambda_{i+1}$, $ g(S^T \mathbf{x}) = f(\mathbf{x}) $ will be a good approximation, i.e., 

\begin{equation}
f(\mathbf{x}) \approx f( P_r \mathbf{x} )
\end{equation}

Then the Monte Carlo integration can be achieved by sampling over the subspace $V$.

However, initially a certain number, $N$, of samples from $\mathbb{R}^n$ have to be evaluated for identifying the active subspace. $N$ is proportional to $log(n)$ if $\nabla f$ can be acquired during evaluating function $f$, while proportional to $n$ if $\nabla f$ has to be evaluated with finite difference. 

To tackle this challenge, a series of alternative cheap models $ \{f_1, f_2, ..., f_m \} $ can be performed prior to identify/train the subspace, 
in which $\mathbf{y_i} = f_i(\mathbf{x}), f_i:\mathbb{R}^n \to \mathbb{R}^{p_i}$ and $\mathbf{y_i} \in \mathbb{R}^{p_i}$. An active subspace $V_{i,j}$ can be identified for the $j_{th}$ component of $\mathbf{y_i}$. Combining them can lead to an active subspace expected to work for the expensive model $f$. The combination will lead to a subspace whose dimension beyond the acceptance, if the active directions from different alternative model are linear independent. 

However, our goal is to approximate the PDF of $\mathbf{y}=f(\mathbf(x))$ instead of the function $f$ itself, and it contains less information. Therefore, it might be possible to identify a low dimensional subspace from the above active subspace for the alternative models to reach the goal. Next a systematic way for this goal will be presented.



\section{Define the Surrogate Subspace}
\subsection{One dimensional subspace}
Let's start from a simple case, in which $\lambda_1 >> \lambda_2$ for each component of $\mathbf{y_i}$, therefore, we have

\begin{equation}
  y_{i,j} = f_{i,j}(\mathbf{x}) = f_{i,j}(\mathbf{u_{i,j}}\mathbf{u_{i,j}}^T\mathbf{x})
\end{equation}

Before moving forward, we shall show an lemma.

\begin{lemma}
  $\forall \mathbf{v} \in \mathbb{R}^n$ with $ \mathbf{u_{i,j}^T} \mathbf{v}  \neq 0 $, 
  \begin{equation}
    y_{i,j} = f_{i,j}(\mathbf{x}) = f_{i,j}( \frac{\mathbf{v}}{\mathbf{u_{i,j}^T} \mathbf{v}} \mathbf{u_{i,j}}^T\mathbf{x})
  \end{equation}

  \begin{proof}
    $f_{i,j}(\mathbf{x}) = f_{i,j}( \mathbf{u_{i,j}}\mathbf{u_{i,j}}^T\mathbf{x} )$ for any $\mathbf{x}$, in particular for
    $ \mathbf{x} = \frac{\mathbf{v}}{\mathbf{u_{i,j}^T} \mathbf{v}} \mathbf{u_{i,j}}^T\mathbf{z} $ for any $\mathbf{z} \in \mathbb{R}^n$.

    $f_{i,j}(\frac{\mathbf{v}}{\mathbf{u_{i,j}^T} \mathbf{v}} \mathbf{u_{i,j}}^T\mathbf{z}) = 
     f_{i,j}(\mathbf{u_{i,j}} \mathbf{u_{i,j}}^T \frac{\mathbf{v}}{\mathbf{u_{i,j}^T} \mathbf{v}} \mathbf{u_{i,j}}^T\mathbf{z}) = 
     f_{i,j}(\mathbf{u_{i,j}} \mathbf{u_{i,j}}^T\mathbf{z}) =
     f_{i,j}(\mathbf{z})
    $    

  \end{proof}
\end{lemma}

Then if $\exists \mathbf{v} \in \mathbb{R}^n$, s.t. $\mathbf{u_{i,j}^T} \mathbf{v}  = 1$ for $\forall i=1,\dots,m; j=1,\dots,p_i$ , equivalently,

\begin{equation} \label{eq:v}
  \underbrace{
    \begin{bmatrix}
      \mathbf{u_{1,1}}^T \\
      \mathbf{u_{1,2}}^T \\
      \vdots \\
      \mathbf{u_{1,p_1}}^T \\
      \vdots \\
      \mathbf{u_{m,1}}^T \\
      \vdots \\
      \mathbf{u_{m,p_m}}^T
    \end{bmatrix}
  }_{A^T}
  \mathbf{v} = 
  \underbrace{
  \begin{bmatrix}
    1 \\
    1 \\
    \vdots \\
    1 \\
    \vdots \\
    1 \\
    \vdots \\
    1
  \end{bmatrix}
  }_{\omega}
\end{equation} \\

and \\

\begin{equation}
    AA^T\mathbf{v} = A\mathbf{\omega}
\end{equation}

we will have 

\begin{equation}
    y_{i,j} = f_{i,j}(\mathbf{x}) = f_{i,j}( {\mathbf{v}}\mathbf{u_{i,j}}^T\mathbf{x})
\end{equation}

If $rank(AA^T) = n $, then the unique solution will be 

\begin{equation} \label{eq:sol1}
\mathbf{v} = (AA^T)^{-1}A\omega.
\end{equation}

If $rank(AA^T) = m < n $, there will be infinite possible solutions for $\mathbf{v}$. If $rank(AA^T) = m < n $, then one solution within the space spanned by $A$ is

\begin{equation} \label{eq:sol2}
  \mathbf{v} = A(A^TA)^{-1}\omega
\end{equation}

In addition, for any new object function $f_i$, either an cheap alternative model or an expensive model. As long as it has an one-dimensional active subspace, and its active direction belongs to the subspace spanned by the above active directions from the training datasets, the above surrogate subspace will also valid for it. In particular, if $rank(AA^T) = n $, then the above surrogate subspace from the training datasets will valid for any objective function $f_i$ as long as there exists an one-dimensional active subspace for it.

Next, we shall consider the case where each component of $\mathbf{x} = [x_i, x_2, \dots, x_n]^T$ follows a standard normal distribution, i.e., $ x_i \sim N(0,1) $.
Then for any $\mathbf{v} \in \mathbb{R}^n$, $ \mathbf{u_{i,j}x} \sim N(0,1) $ and $ \mathbf{vx}/||\mathbf{v}|| \sim N(0,1) $. Therefore,
\begin{equation}
    PDF[y_{i,j} = f_{i,j}(\mathbf{x})] = PDF[f_{i,j}( \mathbf{v} \mathbf{u_{i,j}}^T\mathbf{x})] = PDF[f_{i,j}( \mathbf{v}\frac{\mathbf{v}^T}{||\mathbf{v}||}\mathbf{x})]
\end{equation}

In practice, the domain $\mathcal{X}$ might be bounded. Lets take an example that $ x_i $ is bounded within the three times of the standard deviation, i.e., $x_i \in [-3, 3]$. The bound leads to an additional constrain to Eq. \ref{eq:v}, i.e., $|\mathbf{v}|_{\infty} \leq 1 $. This constrain might not be enabled if the matrix of $AA^T$ is ill-conditioned, i.e., $\kappa (AA^T) = \frac{\lambda_{max}(AA^T)}{\lambda_{min}(AA^T)} >> 1$. Then an iterative pre-selection of the active directions $ \mathbf{u_i} $ to be included into matrix $A$, illustrated in the algorithm \ref{alg:1} can be performed. The active directions which are not included in $A$ will be close to the subspace spanned by $A$ can thus the PDF of corresponding $y_i$ will also be well approximated.

While it is difficult to decide the criteria for judging if the matrix is ill-conditioned. Evaluation of the performance of adding new active direction to $A$ can be performed to decide the best choice of criteria. The evaluation involves the Jensen-Shannon Divergence (JSD) to measure the difference between two PDFs. The JSD between $\rho_1 (y)$ and $\rho_2 (y)$ is defined as

\begin{equation}
	JSD(\rho_1, \rho_2) = \frac{1}{2} \int \rho_1 \log(\frac{\rho_1}{\rho_M}) + \rho_2 \log(\frac{\rho_2}{\rho_M}) dy
\end{equation}

in which $\rho_M = \frac{1}{2}(\rho_1 + \rho_2)$.

\begin{algorithm}
\caption{Iteration by Checking $\kappa (AA^T)$}
\label{alg:1}
\begin{algorithmic}
\STATE{Define $j:=\{ 1,\ldots,m \}$}

\STATE{$A \gets [\mathbf{u_1}] $}

\WHILE{$j \leq m$}

\STATE\label{line3}{$B \gets [A; \mathbf{u_j}] $}

\IF { $\kappa(BB^T) \leq M$ }
  \STATE{ $A \gets B$ }
\ENDIF

\ENDWHILE
\RETURN $T$ 

\STATE{ \it Note: $M$ is the criteria for judging if the matrix is ill-conditioned. \rm }

\end{algorithmic} 
\end{algorithm}

\subsection{Extended to higher dimensional subspace}
Next we shall work with the case where $ \lambda _{i,j,r_{i,j}} >> \lambda _{i,j,r_{i,j}+1} $ for the $j$th component of $\mathbf{y_i}$ and the corresponding eigenvectors 
$\{\mathbf{u_{i,j,1}ï¼Œu_{i,j,2}, \dots, u_{i,j,r_{i,j}}}\}$, then we have
\begin{equation}
  y_{i,j} = f_{i,j}(\mathbf{x}) = f_{i,j}( \sum_{k=1}^{r_{i,j}} [\mathbf{u_{i,j,k}}\mathbf{u_{i,j,k}}^T\mathbf{x}])
\end{equation}

Then if $\exists \mathbf{v_1},\mathbf{v_2}, \dots, \mathbf{v_d}  \in \mathbb{R}^n$, $d =max\{r_{i,j} | 1,\dots,m; j=1,\dots,p_i\}$, 
s.t. $\mathbf{u_{i,j,k}^T} \mathbf{v_{k}}  = 1$ for $\forall i=1,\dots,m; j=1,\dots,p_i; k=1,\dots,d$ , equivalently,

\begin{equation}
  \underbrace{
    \begin{bmatrix}
      \mathbf{u_{1,1,1}}^T \\
      \mathbf{u_{1,2,1}}^T \\
      \vdots \\
      \mathbf{u_{1,p_1,1}}^T \\
      \vdots \\
      \mathbf{u_{m,1,1}}^T \\
      \vdots \\
      \mathbf{u_{m,p_m,1}}^T
    \end{bmatrix}
  }_{A_1^T}
  \mathbf{v_1} = 
  \underbrace{
  \begin{bmatrix}
    1 \\
    1 \\
    \vdots \\
    1 \\
    \vdots \\
    1 \\
    \vdots \\
    1
  \end{bmatrix}
  }_{\omega_1}
\end{equation}

\begin{center}{$\vdots$}\end{center}

\begin{equation}
  \underbrace{
    \begin{bmatrix}
      \mathbf{u_{1,1,r_d}}^T \\
      \mathbf{u_{1,2,r_d}}^T \\
      \vdots \\
      \mathbf{u_{1,p_1,r_d}}^T \\
      \vdots \\
      \mathbf{u_{m,1,r_d}}^T \\
      \vdots \\
      \mathbf{u_{m,p_m,r_d}}^T
    \end{bmatrix}
  }_{A_d^T}
  \mathbf{v_d} = 
  \underbrace{
  \begin{bmatrix}
    1 \\
    1 \\
    \vdots \\
    1 \\
    \vdots \\
    1 \\
    \vdots \\
    1
  \end{bmatrix}
  }_{\omega_d}
\end{equation}

The solutions for $\mathbf{v_k}$ follows equations \ref {eq:sol1} and \ref {eq:sol2}.

we will have 
\begin{equation}
    y_{i,j} = f_{i,j}(\mathbf{x}) = f_{i,j}( \sum_{k=1}^{d} \mathbf{v_k}\mathbf{u_{i,j,k}}^T\mathbf{x})
\end{equation}

Under the special case where $ x_i \sim N(0,1), i=1,\dots,n $. Then we have\\

$[\mathbf{u_{i,j,1}}^T\mathbf{x}, \mathbf{u_{i,j,2}}^T\mathbf{x}, \dots, \mathbf{u_{i,j,r_{i,j}}}^T\mathbf{x}] \sim N( [0, 0, \dots, 0], [1, 1, \dots, 1]) $, \\

If all of $\{ \mathbf{v_k}, k=1,\dots,d \}$ are orthogonal, then we will have \\

$[\frac{\mathbf{v_1}^T \mathbf{x}}{||\mathbf{v_1}||}, \frac{\mathbf{v_2}^T \mathbf{x}}{||\mathbf{v_2}||}, \dots, \frac{\mathbf{v_d}^T \mathbf{x}}{||\mathbf{v_d}||}] \sim 
N([0, 0, \dots, 0], [1, 1, \dots, 1])$ \\

Therefore,
\begin{equation}
    PDF[y_{i,j} = f_{i,j}(\mathbf{x})] = PDF[f_{i,j}( \sum_{k=1}^{d} \mathbf{v_k}\frac{\mathbf{v_k}^T\mathbf{x}}{||\mathbf{v_k}||})]
\end{equation}


\section{Demonstrain} % (fold)
\label{sec:demonstrain}

\subsection{Hydrogen Combustion with 33 Parameters}
In this application, a one dimensional surrogate subspace for a 33-steps detailed Hydrogen reaction mechanism will be demonstrated.

% section demonstrain (end)

\bibliographystyle{siamplain}
\bibliography{references}
\end{document}
